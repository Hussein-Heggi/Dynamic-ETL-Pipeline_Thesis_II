{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 23:02:00 | INFO    | Found 25 CSV files\n",
      "2025-10-13 23:02:01 | INFO    | [1/25] AAPL.csv — capped 987,754 -> 10,000\n",
      "2025-10-13 23:02:01 | INFO    | [1/25] AAPL.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:01 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:01 | INFO    | [AAPL.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:01 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:04 | INFO    | [2/25] AMZN.csv — capped 824,787 -> 10,000\n",
      "2025-10-13 23:02:04 | INFO    | [2/25] AMZN.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:04 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:04 | INFO    | [AMZN.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:04 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:07 | INFO    | [3/25] BA.csv — capped 664,841 -> 10,000\n",
      "2025-10-13 23:02:07 | INFO    | [3/25] BA.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:07 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:07 | INFO    | [BA.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:07 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:10 | INFO    | [4/25] BAC.csv — capped 711,603 -> 10,000\n",
      "2025-10-13 23:02:10 | INFO    | [4/25] BAC.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:10 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:10 | INFO    | [BAC.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:10 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:13 | INFO    | [5/25] C.csv — capped 602,400 -> 10,000\n",
      "2025-10-13 23:02:13 | INFO    | [5/25] C.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:13 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:13 | INFO    | [C.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:13 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:16 | INFO    | [6/25] CAT.csv — capped 515,453 -> 10,000\n",
      "2025-10-13 23:02:16 | INFO    | [6/25] CAT.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:16 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:16 | INFO    | [CAT.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:16 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:19 | INFO    | [7/25] COP.csv — capped 524,679 -> 10,000\n",
      "2025-10-13 23:02:19 | INFO    | [7/25] COP.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:19 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:19 | INFO    | [COP.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:19 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:22 | INFO    | [8/25] COST.csv — capped 507,316 -> 10,000\n",
      "2025-10-13 23:02:22 | INFO    | [8/25] COST.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:22 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:22 | INFO    | [COST.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:22 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:25 | INFO    | [9/25] CVX.csv — capped 565,757 -> 10,000\n",
      "2025-10-13 23:02:25 | INFO    | [9/25] CVX.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:25 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:25 | INFO    | [CVX.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:25 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:28 | INFO    | [10/25] GOOGL.csv — capped 708,987 -> 10,000\n",
      "2025-10-13 23:02:28 | INFO    | [10/25] GOOGL.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:28 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:28 | INFO    | [GOOGL.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:28 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:31 | INFO    |   Progress: 10/25 files | pairs so far=1,000,000 (pos=100,000, neg=900,000)\n",
      "2025-10-13 23:02:31 | INFO    | [11/25] GS.csv — capped 516,742 -> 10,000\n",
      "2025-10-13 23:02:31 | INFO    | [11/25] GS.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:31 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:31 | INFO    | [GS.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:31 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:34 | INFO    | [12/25] HD.csv — capped 518,910 -> 10,000\n",
      "2025-10-13 23:02:34 | INFO    | [12/25] HD.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:34 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:34 | INFO    | [HD.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:34 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:37 | INFO    | [13/25] HON.csv — capped 502,235 -> 10,000\n",
      "2025-10-13 23:02:37 | INFO    | [13/25] HON.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:37 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:37 | INFO    | [HON.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:37 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:40 | INFO    | [14/25] JNJ.csv — capped 528,457 -> 10,000\n",
      "2025-10-13 23:02:40 | INFO    | [14/25] JNJ.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:40 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:40 | INFO    | [JNJ.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:40 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:43 | INFO    | [15/25] JPM.csv — capped 578,581 -> 10,000\n",
      "2025-10-13 23:02:43 | INFO    | [15/25] JPM.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:43 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:43 | INFO    | [JPM.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:43 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:46 | INFO    | [16/25] META.csv — capped 557,072 -> 10,000\n",
      "2025-10-13 23:02:46 | INFO    | [16/25] META.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:46 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:46 | INFO    | [META.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:46 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:49 | INFO    | [17/25] MRK.csv — capped 538,258 -> 10,000\n",
      "2025-10-13 23:02:49 | INFO    | [17/25] MRK.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:49 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:49 | INFO    | [MRK.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:50 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:52 | INFO    | [18/25] MS.csv — capped 533,814 -> 10,000\n",
      "2025-10-13 23:02:52 | INFO    | [18/25] MS.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:52 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:52 | INFO    | [MS.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:53 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:55 | INFO    | [19/25] MSFT.csv — capped 774,618 -> 10,000\n",
      "2025-10-13 23:02:55 | INFO    | [19/25] MSFT.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:55 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:56 | INFO    | [MSFT.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:56 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:02:59 | INFO    | [20/25] NVDA.csv — capped 979,075 -> 10,000\n",
      "2025-10-13 23:02:59 | INFO    | [20/25] NVDA.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:02:59 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:02:59 | INFO    | [NVDA.csv] Split: A=4, B=4\n",
      "2025-10-13 23:02:59 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:03:01 | INFO    |   Progress: 20/25 files | pairs so far=2,000,000 (pos=200,000, neg=1,800,000)\n",
      "2025-10-13 23:03:02 | INFO    | [21/25] PFE.csv — capped 723,840 -> 10,000\n",
      "2025-10-13 23:03:02 | INFO    | [21/25] PFE.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:03:02 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:03:02 | INFO    | [PFE.csv] Split: A=4, B=4\n",
      "2025-10-13 23:03:02 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:03:05 | INFO    | [22/25] T.csv — capped 666,832 -> 10,000\n",
      "2025-10-13 23:03:05 | INFO    | [22/25] T.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:03:05 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:03:05 | INFO    | [T.csv] Split: A=4, B=4\n",
      "2025-10-13 23:03:05 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:03:08 | INFO    | [23/25] VZ.csv — capped 601,452 -> 10,000\n",
      "2025-10-13 23:03:08 | INFO    | [23/25] VZ.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:03:08 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:03:08 | INFO    | [VZ.csv] Split: A=4, B=4\n",
      "2025-10-13 23:03:08 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:03:11 | INFO    | [24/25] WMT.csv — capped 579,428 -> 10,000\n",
      "2025-10-13 23:03:11 | INFO    | [24/25] WMT.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:03:11 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:03:11 | INFO    | [WMT.csv] Split: A=4, B=4\n",
      "2025-10-13 23:03:11 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:03:14 | INFO    | [25/25] XOM.csv — capped 638,464 -> 10,000\n",
      "2025-10-13 23:03:14 | INFO    | [25/25] XOM.csv — rows=10,000, cols=8\n",
      "2025-10-13 23:03:14 | INFO    | Converted 1 date columns: ['ts']\n",
      "2025-10-13 23:03:14 | INFO    | [XOM.csv] Split: A=4, B=4\n",
      "2025-10-13 23:03:14 | INFO    | Applied z-score normalization to 8 features\n",
      "2025-10-13 23:03:17 | INFO    | FINAL — pairs=2,500,000 (pos=250,000, neg=2,250,000), A-cols=4, B-cols=4\n",
      "2025-10-13 23:03:34 | INFO    | Combined saved: ../Datasets/Siamese_Train/pairs_AB.csv | rows=2,500,000 | cols=11\n",
      "2025-10-13 23:03:34 | INFO    | Normalization stats saved to ../Datasets/Siamese_Train/pairs_AB_norm_stats.json\n",
      "2025-10-13 23:03:34 | INFO    | Total elapsed: 93.87s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "Saved: ../Datasets/Siamese_Train/pairs_AB.csv\n",
      "Shape: (2500000, 11)\n",
      "A_* columns: 4 | B_* columns: 4\n",
      "\n",
      "Label distribution:\n",
      "         count\n",
      "label         \n",
      "0      2250000\n",
      "1       250000\n",
      "\n",
      "Sample rows:\n",
      "                idA         idB  label  A_volume     A_vwap     A_open  \\\n",
      "103401   AMZN::3401  AMZN::3401      1  0.454418  15.780217  15.756851   \n",
      "2167674     T::6408     T::7845      0 -0.264422 -32.888015 -32.897835   \n",
      "202020     BA::2020    BA::2020      1 -0.572847  18.582892  18.696160   \n",
      "\n",
      "           A_close     B_high      B_low      B_ts  B_transactions  \n",
      "103401   15.757568  15.786320  15.760567  0.398547       -0.255348  \n",
      "2167674 -32.868750 -33.202602 -33.171527  2.387220       -0.654134  \n",
      "202020   18.655676  18.672651  18.685645 -0.927760       -0.649567  \n",
      "\n",
      "============================================================\n",
      "CREATING TRAIN/VAL/TEST SPLITS\n",
      "============================================================\n",
      "train | rows=1,875,000 | label counts: {0: 1687500, 1: 187500} | ratio: {0: 0.9, 1: 0.1}\n",
      " val  | rows=250,000 | label counts: {0: 225000, 1: 25000} | ratio: {0: 0.9, 1: 0.1}\n",
      "test  | rows=375,000 | label counts: {0: 337500, 1: 37500} | ratio: {0: 0.9, 1: 0.1}\n",
      "\n",
      "Saved:\n",
      " - ../Datasets/Siamese_Train/train.csv\n",
      " - ../Datasets/Siamese_Train/val.csv\n",
      " - ../Datasets/Siamese_Train/test.csv\n",
      "\n",
      "✓ Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIG — edit as needed\n",
    "# ============================================\n",
    "IN_DIR = \"../Datasets/Ingestor\"\n",
    "COMBINED_OUT = \"../Datasets/Siamese_Train/pairs_AB.csv\"\n",
    "\n",
    "# Core\n",
    "GLOB_PATTERN = \"*.csv\"\n",
    "RECURSIVE = False\n",
    "ID_COL = None\n",
    "NEG_PER_POS = 9  # ✨ RESTORED to 9 - this ratio is required (CHANGED BACK)\n",
    "\n",
    "# Column split options\n",
    "COLUMN_SPLIT_MODE = \"half\"\n",
    "COLUMN_SPLIT_SHUFFLE = False\n",
    "EXPLICIT_COLS_A = []\n",
    "EXPLICIT_COLS_B = []\n",
    "\n",
    "# Row cap per CSV\n",
    "ROW_LIMIT = 10_000\n",
    "ROW_LIMIT_MODE = \"head\"\n",
    "\n",
    "# Date parsing → Unix timestamp\n",
    "DATE_MIN_VALID_FRACTION = 0.50\n",
    "NUMERIC_MIN_VALID_FRACTION = 0.98\n",
    "UNIX_UNIT = \"s\"\n",
    "\n",
    "# Logging\n",
    "LOG_EVERY_FILES = 10\n",
    "LOG_LEVEL = \"INFO\"\n",
    "LOG_FILE = None\n",
    "\n",
    "# ✨ NEW: Feature normalization (ADDED)\n",
    "NORMALIZE_FEATURES = True  # Enable z-score normalization per feature column\n",
    "\n",
    "# Columns to DROP from output\n",
    "DROP_FROM_OUTPUT = [\n",
    "    \"num_nan_a\", \"num_nan_b\", \"num_nan_mismatch\",\n",
    "    \"row_idx_A\", \"row_idx_B\", \"label_type\", \"source_file\"\n",
    "]\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def setup_logging(level: str = \"INFO\", logfile: Optional[str] = None):\n",
    "    lvl = getattr(logging, level.upper(), logging.INFO)\n",
    "    fmt = \"%(asctime)s | %(levelname)-7s | %(message)s\"\n",
    "    datefmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    handlers = [logging.StreamHandler()]\n",
    "    if logfile:\n",
    "        logdir = os.path.dirname(os.path.abspath(logfile))\n",
    "        if logdir:\n",
    "            os.makedirs(logdir, exist_ok=True)\n",
    "        handlers.append(logging.FileHandler(logfile, mode=\"w\", encoding=\"utf-8\"))\n",
    "\n",
    "    try:\n",
    "        logging.basicConfig(level=lvl, format=fmt, datefmt=datefmt, handlers=handlers, force=True)\n",
    "    except TypeError:\n",
    "        root = logging.getLogger()\n",
    "        for h in list(root.handlers):\n",
    "            root.removeHandler(h)\n",
    "        logging.basicConfig(level=lvl, format=fmt, datefmt=datefmt, handlers=handlers)\n",
    "\n",
    "\n",
    "def _clean_numeric_like_text(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Remove common decorators like commas/spaces before numeric coercion.\"\"\"\n",
    "    return (s.astype(str)\n",
    "              .str.replace(\",\", \"\", regex=False)\n",
    "              .str.replace(\" \", \"\", regex=False)\n",
    "              .str.replace(\"\\u00A0\", \"\", regex=False))\n",
    "\n",
    "\n",
    "def detect_numeric_columns(df: pd.DataFrame, *, id_col: Optional[str], min_valid_fraction: float) -> List[str]:\n",
    "    \"\"\"Columns that are numeric or safely numeric after cleaning.\"\"\"\n",
    "    cols = [c for c in df.columns if (id_col is None or c != id_col)]\n",
    "    numeric_cols = []\n",
    "    for c in cols:\n",
    "        s_num = pd.to_numeric(_clean_numeric_like_text(df[c]), errors=\"coerce\")\n",
    "        if s_num.notna().mean() >= min_valid_fraction:\n",
    "            numeric_cols.append(c)\n",
    "    return numeric_cols\n",
    "\n",
    "\n",
    "def datetime_series_to_unix(dt: pd.Series, unit: str) -> pd.Series:\n",
    "    \"\"\"Convert timezone-aware datetime64[ns] Series to Unix timestamps.\"\"\"\n",
    "    arr_dt = dt.to_numpy(dtype=\"datetime64[ns]\")\n",
    "    mask_nat = np.isnat(arr_dt)\n",
    "    arr_ns = arr_dt.astype(\"datetime64[ns]\").astype(\"int64\").astype(\"float64\")\n",
    "    arr_ns[mask_nat] = np.nan\n",
    "    if unit == \"ms\":\n",
    "        return pd.Series(arr_ns / 1e6, index=dt.index, dtype=\"float64\")\n",
    "    else:\n",
    "        return pd.Series(arr_ns / 1e9, index=dt.index, dtype=\"float64\")\n",
    "\n",
    "\n",
    "def convert_only_non_numeric_dates_to_unix(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    id_col: Optional[str],\n",
    "    numeric_min_valid_fraction: float,\n",
    "    date_min_valid_fraction: float,\n",
    "    unit: str,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Only non-numeric columns are tested as dates and converted to Unix.\"\"\"\n",
    "    df = df.copy()\n",
    "    candidate_cols = [c for c in df.columns if (id_col is None or c != id_col)]\n",
    "\n",
    "    numeric_cols = set(detect_numeric_columns(df, id_col=id_col, min_valid_fraction=numeric_min_valid_fraction))\n",
    "    converted = []\n",
    "    for c in candidate_cols:\n",
    "        if c in numeric_cols:\n",
    "            continue\n",
    "        dt = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "        if dt.notna().mean() >= date_min_valid_fraction:\n",
    "            df[c] = datetime_series_to_unix(dt, unit=unit)\n",
    "            converted.append(c)\n",
    "    return df, converted\n",
    "\n",
    "\n",
    "def as_numeric(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Coerce columns to numeric.\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        out[c] = pd.to_numeric(_clean_numeric_like_text(out[c]), errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# ✨ NEW FUNCTION: Normalize features (ADDED)\n",
    "def normalize_features(df: pd.DataFrame, cols: List[str], stats: Optional[dict] = None) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Z-score normalization: (x - mean) / std\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns to normalize\n",
    "        cols: List of column names to normalize\n",
    "        stats: Optional dict with pre-computed means/stds (for test set)\n",
    "    \n",
    "    Returns:\n",
    "        normalized_df, stats_dict\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    if stats is None:\n",
    "        # Compute stats from training data\n",
    "        stats = {}\n",
    "        for c in cols:\n",
    "            mean_val = df[c].mean()\n",
    "            std_val = df[c].std()\n",
    "            if std_val == 0 or pd.isna(std_val):\n",
    "                std_val = 1.0  # Avoid division by zero\n",
    "            stats[c] = {'mean': mean_val, 'std': std_val}\n",
    "    \n",
    "    # Apply normalization\n",
    "    for c in cols:\n",
    "        mean_val = stats[c]['mean']\n",
    "        std_val = stats[c]['std']\n",
    "        df_norm[c] = (df[c] - mean_val) / std_val\n",
    "    \n",
    "    return df_norm, stats\n",
    "\n",
    "\n",
    "def split_numeric_columns(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    id_col: Optional[str],\n",
    "    mode: str = \"half\",\n",
    "    shuffle: bool = False,\n",
    "    seed: int = 42,\n",
    "    explicit_A: Optional[List[str]] = None,\n",
    "    explicit_B: Optional[List[str]] = None,\n",
    "    min_valid_fraction: float = 0.01,\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Return (cols_A, cols_B) disjoint numeric feature lists.\"\"\"\n",
    "    all_cols = df.columns.tolist()\n",
    "    feature_cols = [c for c in all_cols if (id_col is None or c != id_col)]\n",
    "\n",
    "    valid_frac = {}\n",
    "    for c in feature_cols:\n",
    "        s = pd.to_numeric(_clean_numeric_like_text(df[c]), errors=\"coerce\")\n",
    "        valid_frac[c] = s.notna().mean()\n",
    "\n",
    "    numeric_ok = [c for c in feature_cols if valid_frac[c] >= min_valid_fraction]\n",
    "\n",
    "    if explicit_A and explicit_B:\n",
    "        missing = [c for c in (explicit_A + explicit_B) if c not in numeric_ok]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Explicit columns not usable: {missing}\")\n",
    "        overlap = set(explicit_A).intersection(explicit_B)\n",
    "        if overlap:\n",
    "            raise ValueError(f\"Explicit A/B overlap: {overlap}\")\n",
    "        return list(explicit_A), list(explicit_B)\n",
    "\n",
    "    cols = numeric_ok.copy()\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(cols)\n",
    "\n",
    "    dropped = [c for c in feature_cols if c not in numeric_ok]\n",
    "    if dropped:\n",
    "        logging.warning(f\"Dropping {len(dropped)} non-usable column(s): {dropped[:10]}{'...' if len(dropped)>10 else ''}\")\n",
    "\n",
    "    mid = len(cols) // 2\n",
    "    cols_A, cols_B = cols[:mid], cols[mid:]\n",
    "    if len(cols_A) == 0 or len(cols_B) == 0:\n",
    "        raise ValueError(\"Column split resulted in an empty side; need at least two usable numeric columns.\")\n",
    "    return cols_A, cols_B\n",
    "\n",
    "\n",
    "def _apply_row_cap(df: pd.DataFrame, cap: int, mode: str, seed: int) -> pd.DataFrame:\n",
    "    if cap is None or len(df) <= cap:\n",
    "        return df\n",
    "    if mode == \"sample\":\n",
    "        return df.sample(n=cap, random_state=seed)\n",
    "    return df.head(cap)\n",
    "\n",
    "\n",
    "def rows_to_AB_record(\n",
    "    i: int, j: int,\n",
    "    A_df: pd.DataFrame, B_df: pd.DataFrame,\n",
    "    cols_A: List[str], cols_B: List[str],\n",
    "    id_col_eff: str,\n",
    "    label: int,\n",
    "    source_file: str\n",
    ") -> dict:\n",
    "    \"\"\"Construct a single training record with idA, idB, A_<col>..., B_<col>..., label.\"\"\"\n",
    "    rec = {\n",
    "        \"idA\": A_df[id_col_eff].iloc[i],\n",
    "        \"idB\": B_df[id_col_eff].iloc[j],\n",
    "        \"label\": int(label),\n",
    "        \"source_file\": os.path.basename(source_file),\n",
    "        \"row_idx_A\": int(i),\n",
    "        \"row_idx_B\": int(j),\n",
    "    }\n",
    "    for c in cols_A:\n",
    "        rec[f\"A_{c}\"] = A_df[c].iloc[i]\n",
    "    for c in cols_B:\n",
    "        rec[f\"B_{c}\"] = B_df[c].iloc[j]\n",
    "    return rec\n",
    "\n",
    "\n",
    "def build_pairs_from_single_df_column_split_siamese(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    id_col: Optional[str],\n",
    "    negatives_per_positive: int,\n",
    "    seed: int,\n",
    "    source_file: str,\n",
    "    column_split_mode: str = \"half\",\n",
    "    column_split_shuffle: bool = False,\n",
    "    explicit_cols_A: Optional[List[str]] = None,\n",
    "    explicit_cols_B: Optional[List[str]] = None,\n",
    "    normalize: bool = False,  # ✨ NEW PARAMETER (ADDED)\n",
    "    norm_stats: Optional[dict] = None,  # ✨ NEW PARAMETER (ADDED)\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, Optional[dict]]:  # ✨ MODIFIED RETURN TYPE\n",
    "    \"\"\"\n",
    "    One CSV -> Siamese-ready pairs with optional normalization.\n",
    "    Returns: (pairs_df, labels, index_meta, normalization_stats)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Ensure ID\n",
    "    if id_col is None or id_col not in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"_row_id\"] = np.arange(len(df)).astype(str)\n",
    "        stem = os.path.splitext(os.path.basename(source_file))[0]\n",
    "        df[\"_row_id\"] = stem + \"::\" + df[\"_row_id\"]\n",
    "        id_col_eff = \"_row_id\"\n",
    "        logging.debug(f\"[{source_file}] Synthetic ID '_row_id' created\")\n",
    "    else:\n",
    "        id_col_eff = id_col\n",
    "\n",
    "    # Convert date-like (non-numeric) to Unix\n",
    "    df, converted = convert_only_non_numeric_dates_to_unix(\n",
    "        df,\n",
    "        id_col=id_col_eff,\n",
    "        numeric_min_valid_fraction=NUMERIC_MIN_VALID_FRACTION,\n",
    "        date_min_valid_fraction=DATE_MIN_VALID_FRACTION,\n",
    "        unit=UNIX_UNIT,\n",
    "    )\n",
    "    if converted:\n",
    "        logging.info(f\"Converted {len(converted)} date columns: {converted}\")\n",
    "\n",
    "    # Split numeric columns\n",
    "    cols_A, cols_B = split_numeric_columns(\n",
    "        df, id_col=id_col_eff,\n",
    "        mode=column_split_mode, shuffle=column_split_shuffle, seed=seed,\n",
    "        explicit_A=explicit_cols_A, explicit_B=explicit_cols_B,\n",
    "    )\n",
    "    logging.info(f\"[{os.path.basename(source_file)}] Split: A={len(cols_A)}, B={len(cols_B)}\")\n",
    "\n",
    "    # Coerce to numeric\n",
    "    df_num = as_numeric(df, cols_A + cols_B)\n",
    "\n",
    "    # ✨ NEW: Normalize features (ADDED)\n",
    "    if normalize:\n",
    "        df_num, norm_stats = normalize_features(df_num, cols_A + cols_B, stats=norm_stats)\n",
    "        logging.info(f\"Applied z-score normalization to {len(cols_A + cols_B)} features\")\n",
    "    else:\n",
    "        norm_stats = None\n",
    "\n",
    "    # Build A/B sides\n",
    "    A_side = df_num[[id_col_eff] + cols_A].copy()\n",
    "    B_side = df_num[[id_col_eff] + cols_B].copy()\n",
    "\n",
    "    n = len(df_num)\n",
    "    if n < 2:\n",
    "        raise ValueError(\"Need at least 2 rows to form negatives.\")\n",
    "\n",
    "    # Build pairs\n",
    "    records = []\n",
    "    pos_pairs, neg_pairs = [], []\n",
    "    all_indices = np.arange(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        pos_pairs.append((i, i))\n",
    "        size = min(negatives_per_positive, n - 1)\n",
    "        if size > 0:\n",
    "            candidates = np.delete(all_indices, i)\n",
    "            choices = rng.choice(candidates, size=size, replace=False)\n",
    "            for j in choices:\n",
    "                neg_pairs.append((i, j))\n",
    "\n",
    "    # Emit records\n",
    "    for (i, j) in pos_pairs:\n",
    "        records.append(rows_to_AB_record(i, j, A_side, B_side, cols_A, cols_B, id_col_eff, 1, source_file))\n",
    "    for (i, j) in neg_pairs:\n",
    "        records.append(rows_to_AB_record(i, j, A_side, B_side, cols_A, cols_B, id_col_eff, 0, source_file))\n",
    "\n",
    "    out_df = pd.DataFrame.from_records(records)\n",
    "    y = out_df[\"label\"].astype(int).rename(\"label\")\n",
    "    idx = out_df[[\"idA\", \"idB\", \"row_idx_A\", \"row_idx_B\", \"source_file\"]].copy()\n",
    "\n",
    "    return out_df.drop(columns=[\"row_idx_A\", \"row_idx_B\", \"source_file\"]), y, idx, norm_stats\n",
    "\n",
    "\n",
    "def save_siamese_pairs(out_df: pd.DataFrame, y: pd.Series, idx: pd.DataFrame, out_path: str, drop_cols: List[str]):\n",
    "    \"\"\"Save combined pairs dataset.\"\"\"\n",
    "    df_combined = out_df.copy()\n",
    "    if \"label\" not in df_combined.columns:\n",
    "        df_combined = pd.concat([df_combined, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    drop_present = [c for c in drop_cols if c in df_combined.columns]\n",
    "    if drop_present:\n",
    "        df_combined = df_combined.drop(columns=drop_present)\n",
    "        logging.info(f\"Dropped from output: {drop_present}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_path)) or \".\", exist_ok=True)\n",
    "    ext = os.path.splitext(out_path)[1].lower()\n",
    "    if ext == \".parquet\":\n",
    "        df_combined.to_parquet(out_path, index=False)\n",
    "    else:\n",
    "        df_combined.to_csv(out_path, index=False)\n",
    "    logging.info(f\"Combined saved: {out_path} | rows={len(df_combined):,} | cols={df_combined.shape[1]:,}\")\n",
    "\n",
    "\n",
    "def build_pairs_from_dir_column_split_siamese(\n",
    "    in_dir: str,\n",
    "    *,\n",
    "    glob_pattern: str = \"*.csv\",\n",
    "    recursive: bool = False,\n",
    "    id_col: Optional[str] = None,\n",
    "    negatives_per_positive: int = 9,\n",
    "    seed: int = 42,\n",
    "    combined_out: str = \"../Datasets/Siamese_Train/pairs_AB.csv\",\n",
    "    log_every_files: int = 10,\n",
    "    row_limit: Optional[int] = None,\n",
    "    row_limit_mode: str = \"head\",\n",
    "    column_split_mode: str = \"half\",\n",
    "    column_split_shuffle: bool = False,\n",
    "    explicit_cols_A: Optional[List[str]] = None,\n",
    "    explicit_cols_B: Optional[List[str]] = None,\n",
    "    normalize: bool = False,  # ✨ NEW PARAMETER (ADDED)\n",
    "):\n",
    "    \"\"\"Multi-CSV orchestrator for Siamese encoder with optional normalization.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    search = os.path.join(in_dir, \"**\", glob_pattern) if recursive else os.path.join(in_dir, glob_pattern)\n",
    "    files = sorted(glob.glob(search, recursive=recursive))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files matched: {search}\")\n",
    "    logging.info(f\"Found {len(files):,} CSV files\")\n",
    "\n",
    "    all_rows = []\n",
    "    total_pos, total_neg = 0, 0\n",
    "    norm_stats = None  # ✨ NEW: Track normalization stats (ADDED)\n",
    "\n",
    "    for k, f in enumerate(files, 1):\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        original_rows = len(df)\n",
    "        df = _apply_row_cap(df, row_limit, row_limit_mode, seed)\n",
    "        if len(df) < original_rows:\n",
    "            logging.info(f\"[{k}/{len(files)}] {os.path.basename(f)} — capped {original_rows:,} -> {len(df):,}\")\n",
    "\n",
    "        if len(df) < 2:\n",
    "            logging.warning(f\"[{k}/{len(files)}] {os.path.basename(f)} has <2 rows; skipping.\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"[{k}/{len(files)}] {os.path.basename(f)} — rows={len(df):,}, cols={len(df.columns):,}\")\n",
    "\n",
    "        # ✨ MODIFIED: Pass normalize parameter and get stats back\n",
    "        out_df, y, _idx_meta, file_norm_stats = build_pairs_from_single_df_column_split_siamese(\n",
    "            df=df,\n",
    "            id_col=id_col,\n",
    "            negatives_per_positive=negatives_per_positive,\n",
    "            seed=seed,\n",
    "            source_file=f,\n",
    "            column_split_mode=column_split_mode,\n",
    "            column_split_shuffle=column_split_shuffle,\n",
    "            explicit_cols_A=explicit_cols_A,\n",
    "            explicit_cols_B=explicit_cols_B,\n",
    "            normalize=normalize,\n",
    "            norm_stats=norm_stats if k > 1 else None,  # Use stats from first file\n",
    "        )\n",
    "\n",
    "        # ✨ NEW: Store normalization stats from first file (ADDED)\n",
    "        if k == 1 and normalize:\n",
    "            norm_stats = file_norm_stats\n",
    "\n",
    "        all_rows.append(out_df)\n",
    "        pos = int(y.sum())\n",
    "        neg = len(y) - pos\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "\n",
    "        if k % log_every_files == 0:\n",
    "            logging.info(f\"  Progress: {k:,}/{len(files):,} files | \"\n",
    "                         f\"pairs so far={total_pos+total_neg:,} (pos={total_pos:,}, neg={total_neg:,})\")\n",
    "\n",
    "    if not all_rows:\n",
    "        raise RuntimeError(\"No valid CSVs produced pairs.\")\n",
    "\n",
    "    final_df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "    logging.info(\n",
    "        f\"FINAL — pairs={len(final_df):,} \"\n",
    "        f\"(pos={int(final_df['label'].sum()):,}, neg={len(final_df)-int(final_df['label'].sum()):,}), \"\n",
    "        f\"A-cols={len([c for c in final_df.columns if c.startswith('A_')])}, \"\n",
    "        f\"B-cols={len([c for c in final_df.columns if c.startswith('B_')])}\"\n",
    "    )\n",
    "\n",
    "    save_siamese_pairs(final_df, final_df[\"label\"], final_df[[\"idA\",\"idB\"]], combined_out, drop_cols=DROP_FROM_OUTPUT)\n",
    "\n",
    "    # ✨ NEW: Save normalization stats (ADDED)\n",
    "    if normalize and norm_stats:\n",
    "        import json\n",
    "        stats_path = combined_out.replace('.csv', '_norm_stats.json')\n",
    "        with open(stats_path, 'w') as f:\n",
    "            json.dump(norm_stats, f, indent=2)\n",
    "        logging.info(f\"Normalization stats saved to {stats_path}\")\n",
    "\n",
    "    logging.info(f\"Total elapsed: {time.perf_counter() - t0:.2f}s\")\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "setup_logging(LOG_LEVEL, LOG_FILE)\n",
    "\n",
    "pairs_df = build_pairs_from_dir_column_split_siamese(\n",
    "    in_dir=IN_DIR,\n",
    "    glob_pattern=GLOB_PATTERN,\n",
    "    recursive=RECURSIVE,\n",
    "    id_col=ID_COL,\n",
    "    negatives_per_positive=NEG_PER_POS,\n",
    "    seed=42,\n",
    "    combined_out=COMBINED_OUT,\n",
    "    log_every_files=LOG_EVERY_FILES,\n",
    "    row_limit=ROW_LIMIT,\n",
    "    row_limit_mode=ROW_LIMIT_MODE,\n",
    "    column_split_mode=COLUMN_SPLIT_MODE,\n",
    "    column_split_shuffle=COLUMN_SPLIT_SHUFFLE,\n",
    "    explicit_cols_A=EXPLICIT_COLS_A,\n",
    "    explicit_cols_B=EXPLICIT_COLS_B,\n",
    "    normalize=NORMALIZE_FEATURES,  # ✨ NEW: Enable normalization (ADDED)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Saved: {COMBINED_OUT}\")\n",
    "print(f\"Shape: {pairs_df.shape}\")\n",
    "\n",
    "# Column summary\n",
    "cols = list(pairs_df.columns)\n",
    "num_A = sum(c.startswith(\"A_\") for c in cols)\n",
    "num_B = sum(c.startswith(\"B_\") for c in cols)\n",
    "print(f\"A_* columns: {num_A} | B_* columns: {num_B}\")\n",
    "\n",
    "# Label balance\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(pairs_df[\"label\"].value_counts(dropna=False).to_frame(\"count\"))\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(pairs_df.sample(min(3, len(pairs_df)), random_state=0))\n",
    "\n",
    "# ===========================\n",
    "# TRAIN/VAL/TEST SPLIT\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TRAIN/VAL/TEST SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TRAIN_FRACTION = 0.75\n",
    "VAL_FRACTION   = 0.10\n",
    "TEST_FRACTION  = 0.15\n",
    "SEED = 42\n",
    "\n",
    "assert abs(TRAIN_FRACTION + VAL_FRACTION + TEST_FRACTION - 1.0) < 1e-9\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "OUT_DIR = os.path.dirname(COMBINED_OUT)\n",
    "y = pairs_df[\"label\"]\n",
    "\n",
    "# First split: train vs temp\n",
    "test_val_size = VAL_FRACTION + TEST_FRACTION\n",
    "train_df, temp_df = train_test_split(\n",
    "    pairs_df, test_size=test_val_size, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: val vs test\n",
    "rel_val = VAL_FRACTION / (VAL_FRACTION + TEST_FRACTION)\n",
    "temp_y = temp_df[\"label\"]\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=(1 - rel_val), random_state=SEED, stratify=temp_y\n",
    ")\n",
    "\n",
    "# Summary\n",
    "def summarize(split_name, d):\n",
    "    counts = d[\"label\"].value_counts(dropna=False).sort_index()\n",
    "    pct = (counts / len(d)).round(4)\n",
    "    print(f\"{split_name:>5} | rows={len(d):,} | label counts: {counts.to_dict()} | ratio: {pct.to_dict()}\")\n",
    "\n",
    "summarize(\"train\", train_df)\n",
    "summarize(\" val \", val_df)\n",
    "summarize(\"test \", test_df)\n",
    "\n",
    "# Save splits\n",
    "train_path = os.path.join(OUT_DIR, \"train.csv\")\n",
    "val_path   = os.path.join(OUT_DIR, \"val.csv\")\n",
    "test_path  = os.path.join(OUT_DIR, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(f\" - {train_path}\")\n",
    "print(f\" - {val_path}\")\n",
    "print(f\" - {test_path}\")\n",
    "print(\"\\n✓ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 1875000 pairs from ../Datasets/Siamese_Train/train.csv\n",
      "  A features: 4, B features: 4\n",
      "  Positive pairs: 187500 (10.0%)\n",
      "Loaded 250000 pairs from ../Datasets/Siamese_Train/val.csv\n",
      "  A features: 4, B features: 4\n",
      "  Positive pairs: 25000 (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g7/Desktop/Thesis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 375000 pairs from ../Datasets/Siamese_Train/test.csv\n",
      "  A features: 4, B features: 4\n",
      "  Positive pairs: 37500 (10.0%)\n",
      "\n",
      "Model architecture:\n",
      "  Tower A input: 4 features\n",
      "  Tower B input: 4 features\n",
      "  Hidden layers: [64, 32]\n",
      "  Embedding dim: 16\n",
      "  Total parameters: 9,057\n",
      "\n",
      "⚠️  Class imbalance ratio: 9.00:1 (negatives:positives)\n",
      "   Strategy: Focal Loss\n",
      "   Focal Loss - alpha=0.9, gamma=3.0\n",
      "   This focuses learning on hard-to-classify examples\n",
      "\n",
      "Starting training for 100 epochs...\n",
      "Batch size: 128, LR: 0.01\n",
      "Monitoring F1 score for early stopping (patience=20)\n",
      "Target: F1 > 0.60 for good performance with 9:1 imbalance\n",
      "\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0156 | Train Acc: 0.4959 | Train F1: 0.1669\n",
      "  Val Loss: 0.0156 | Val Acc: 0.1000\n",
      "  Val Precision: 0.1000 | Val Recall: 1.0000\n",
      "  Val F1: 0.1818 | Val AUC: 0.5000\n",
      "  LR: 0.010000\n",
      "  → Model saved (best val F1: 0.1818)\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0156 | Train Acc: 0.4996 | Train F1: 0.1667\n",
      "  Val Loss: 0.0156 | Val Acc: 0.9000\n",
      "  Val Precision: 0.0000 | Val Recall: 0.0000\n",
      "  Val F1: 0.0000 | Val AUC: 0.5000\n",
      "  LR: 0.010000\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0156 | Train Acc: 0.4949 | Train F1: 0.1666\n",
      "  Val Loss: 0.0156 | Val Acc: 0.1000\n",
      "  Val Precision: 0.1000 | Val Recall: 1.0000\n",
      "  Val F1: 0.1818 | Val AUC: 0.5000\n",
      "  LR: 0.010000\n",
      "\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0156 | Train Acc: 0.4941 | Train F1: 0.1674\n",
      "  Val Loss: 0.0156 | Val Acc: 0.1000\n",
      "  Val Precision: 0.1000 | Val Recall: 1.0000\n",
      "  Val F1: 0.1818 | Val AUC: 0.5000\n",
      "  LR: 0.010000\n",
      "\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0156 | Train Acc: 0.4975 | Train F1: 0.1664\n",
      "  Val Loss: 0.0156 | Val Acc: 0.9000\n",
      "  Val Precision: 0.0000 | Val Recall: 0.0000\n",
      "  Val F1: 0.0000 | Val AUC: 0.5000\n",
      "  LR: 0.010000\n",
      "  ⚠️  WARNING: F1 is near zero - model may be predicting all negatives!\n",
      "     Consider: Lower LR, increase focal gamma, or check data\n",
      "\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 639\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# RUN TRAINING\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     model, history = \u001b[43mtrain_siamese_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     plot_training_history(history)\n\u001b[32m    642\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 441\u001b[39m, in \u001b[36mtrain_siamese_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m train_loss, train_acc, train_f1 = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    444\u001b[39m val_loss, val_acc, val_prec, val_rec, val_f1, val_auc = evaluate(model, val_loader, criterion, DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 260\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m     loss = criterion(preds, labels)\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Gradient clipping to prevent exploding gradients\u001b[39;00m\n\u001b[32m    263\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Thesis/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SIAMESE ENCODER MODEL\n",
    "# ============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# CONFIG\n",
    "# ============================================\n",
    "DATA_DIR = \"../Datasets/Siamese_Train\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.csv\"\n",
    "VAL_PATH = f\"{DATA_DIR}/val.csv\"\n",
    "TEST_PATH = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "# Model architecture\n",
    "HIDDEN_DIMS = [64, 32]         # ✨ SIMPLIFIED - smaller model learns faster\n",
    "EMBEDDING_DIM = 16             # ✨ REDUCED embedding size\n",
    "DROPOUT = 0.2                  # ✨ REDUCED dropout - was preventing learning\n",
    "USE_BATCH_NORM = False         # ✨ DISABLED - can cause issues with small batches\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128               # ✨ SMALLER batches for more frequent updates\n",
    "LEARNING_RATE = 0.01           # ✨ MUCH HIGHER LR to force learning\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE = 20                  \n",
    "WEIGHT_DECAY = 0               # ✨ DISABLED - no regularization until model learns\n",
    "\n",
    "# ✨ AGGRESSIVE focal loss parameters\n",
    "USE_FOCAL_LOSS = True\n",
    "FOCAL_ALPHA = 0.9              # ✨ INCREASED - 90% focus on positives\n",
    "FOCAL_GAMMA = 3.0              # ✨ INCREASED - more aggressive focus on hard examples\n",
    "\n",
    "# ✨ NEW: Class weights for even more aggressive balancing\n",
    "USE_CLASS_WEIGHTS = True       # Apply additional class weights\n",
    "POSITIVE_CLASS_WEIGHT = 15.0   # Weight positive samples 15x (beyond focal loss)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "MODEL_SAVE_PATH = f\"{DATA_DIR}/siamese_model.pt\"\n",
    "RESULTS_PATH = f\"{DATA_DIR}/training_results.csv\"\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DATASET CLASS\n",
    "# ============================================\n",
    "class SiamesePairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Siamese network training.\n",
    "    Loads pairs with A_* features, B_* features, and label.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, a_cols=None, b_cols=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Auto-detect A_* and B_* columns if not provided\n",
    "        if a_cols is None:\n",
    "            self.a_cols = [c for c in self.df.columns if c.startswith(\"A_\")]\n",
    "        else:\n",
    "            self.a_cols = a_cols\n",
    "            \n",
    "        if b_cols is None:\n",
    "            self.b_cols = [c for c in self.df.columns if c.startswith(\"B_\")]\n",
    "        else:\n",
    "            self.b_cols = b_cols\n",
    "        \n",
    "        # Extract features and labels\n",
    "        self.X_a = self.df[self.a_cols].values.astype(np.float32)\n",
    "        self.X_b = self.df[self.b_cols].values.astype(np.float32)\n",
    "        self.y = self.df[\"label\"].values.astype(np.float32)\n",
    "        \n",
    "        # Handle NaNs: replace with column mean\n",
    "        self._impute_nans()\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} pairs from {csv_path}\")\n",
    "        print(f\"  A features: {len(self.a_cols)}, B features: {len(self.b_cols)}\")\n",
    "        print(f\"  Positive pairs: {self.y.sum():.0f} ({100*self.y.mean():.1f}%)\")\n",
    "    \n",
    "    def _impute_nans(self):\n",
    "        \"\"\"Replace NaNs with column means.\"\"\"\n",
    "        # Compute means ignoring NaNs\n",
    "        a_means = np.nanmean(self.X_a, axis=0)\n",
    "        b_means = np.nanmean(self.X_b, axis=0)\n",
    "        \n",
    "        # Replace NaNs\n",
    "        a_nan_mask = np.isnan(self.X_a)\n",
    "        b_nan_mask = np.isnan(self.X_b)\n",
    "        \n",
    "        for i in range(self.X_a.shape[1]):\n",
    "            self.X_a[a_nan_mask[:, i], i] = a_means[i]\n",
    "        \n",
    "        for i in range(self.X_b.shape[1]):\n",
    "            self.X_b[b_nan_mask[:, i], i] = b_means[i]\n",
    "        \n",
    "        # If any column is entirely NaN, set to 0\n",
    "        self.X_a = np.nan_to_num(self.X_a, nan=0.0)\n",
    "        self.X_b = np.nan_to_num(self.X_b, nan=0.0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X_a[idx]),\n",
    "            torch.from_numpy(self.X_b[idx]),\n",
    "            torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MODEL: TWO-TOWER SIAMESE ENCODER\n",
    "# ============================================\n",
    "class TowerNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Single tower: transforms input features to embedding space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, embedding_dim, dropout=0.3, use_batch_norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Final embedding layer (no activation - we want raw embeddings)\n",
    "        layers.append(nn.Linear(prev_dim, embedding_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-tower Siamese architecture:\n",
    "    - Tower A: encodes A_* features\n",
    "    - Tower B: encodes B_* features\n",
    "    - Similarity: computes distance/similarity between embeddings\n",
    "    - Classifier: predicts if pair matches (label=1) or not (label=0)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim_a, input_dim_b, hidden_dims, embedding_dim, \n",
    "                 dropout=0.3, use_batch_norm=True, distance_metric=\"cosine\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.distance_metric = distance_metric\n",
    "        \n",
    "        # Two towers (can share weights or be separate - here separate)\n",
    "        self.tower_a = TowerNetwork(input_dim_a, hidden_dims, embedding_dim, dropout, use_batch_norm)\n",
    "        self.tower_b = TowerNetwork(input_dim_b, hidden_dims, embedding_dim, dropout, use_batch_norm)\n",
    "        \n",
    "        # Final classifier: takes similarity features and predicts match probability\n",
    "        # Input: embedding_dim * 3 (concatenated embeddings + element-wise abs diff)\n",
    "        classifier_input = embedding_dim * 3\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_a, x_b):\n",
    "        # Encode both sides\n",
    "        emb_a = self.tower_a(x_a)  # (batch, embedding_dim)\n",
    "        emb_b = self.tower_b(x_b)  # (batch, embedding_dim)\n",
    "        \n",
    "        # Compute similarity features\n",
    "        # Option 1: Concatenate embeddings + absolute difference\n",
    "        abs_diff = torch.abs(emb_a - emb_b)\n",
    "        combined = torch.cat([emb_a, emb_b, abs_diff], dim=1)\n",
    "        \n",
    "        # Predict match probability\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output.squeeze(-1), emb_a, emb_b\n",
    "    \n",
    "    def get_embeddings(self, x_a, x_b):\n",
    "        \"\"\"Extract embeddings without classification.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            emb_a = self.tower_a(x_a)\n",
    "            emb_b = self.tower_b(x_b)\n",
    "        return emb_a, emb_b\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# TRAINING UTILITIES\n",
    "# ============================================\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling extreme class imbalance.\n",
    "    Focuses learning on hard-to-classify examples.\n",
    "    \n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Args:\n",
    "        alpha: Weight for positive class (0.75 means focus on positives)\n",
    "        gamma: Focusing parameter (2.0 is standard, higher = more focus on hard examples)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, preds, labels):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        preds = torch.clamp(preds, min=1e-7, max=1-1e-7)\n",
    "        \n",
    "        # Compute focal loss components\n",
    "        bce = -(labels * torch.log(preds) + (1 - labels) * torch.log(1 - preds))\n",
    "        \n",
    "        # Compute p_t (probability of correct class)\n",
    "        p_t = torch.where(labels == 1, preds, 1 - preds)\n",
    "        \n",
    "        # Compute focal weight: (1 - p_t)^gamma\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Compute alpha weight\n",
    "        alpha_t = torch.where(labels == 1, self.alpha, 1 - self.alpha)\n",
    "        \n",
    "        # Final focal loss\n",
    "        focal_loss = alpha_t * focal_weight * bce\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for x_a, x_b, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x_a, x_b, labels = x_a.to(device), x_b.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds, _, _ = model(x_a, x_b)\n",
    "        \n",
    "        # Handle both function and nn.Module losses\n",
    "        if callable(criterion) and not isinstance(criterion, nn.Module):\n",
    "            loss = criterion(preds, labels)\n",
    "        else:\n",
    "            loss = criterion(preds, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds > 0.5)\n",
    "    \n",
    "    # Also compute F1 on training set to monitor\n",
    "    _, _, train_f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds > 0.5, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return avg_loss, accuracy, train_f1\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"Evaluate on validation/test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_a, x_b, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            x_a, x_b, labels = x_a.to(device), x_b.to(device), labels.to(device)\n",
    "            \n",
    "            preds, _, _ = model(x_a, x_b)\n",
    "            \n",
    "            # Handle both function and nn.Module losses\n",
    "            if callable(criterion) and not isinstance(criterion, nn.Module):\n",
    "                loss = criterion(preds, labels)\n",
    "            else:\n",
    "                loss = criterion(preds, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Metrics with specified threshold\n",
    "    accuracy = accuracy_score(all_labels, all_preds > threshold)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds > threshold, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Only compute AUC if we have both classes\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = 0.5  # If only one class present\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n",
    "\n",
    "\n",
    "def find_optimal_threshold(model, loader, device):\n",
    "    \"\"\"\n",
    "    Find optimal decision threshold for imbalanced data.\n",
    "    Returns threshold that maximizes F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_a, x_b, labels in tqdm(loader, desc=\"Finding threshold\", leave=False):\n",
    "            x_a, x_b, labels = x_a.to(device), x_b.to(device), labels.to(device)\n",
    "            preds, _, _ = model(x_a, x_b)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Try different thresholds\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "        _, _, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_preds > threshold, average='binary', zero_division=0\n",
    "        )\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================\n",
    "def train_siamese_model():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = SiamesePairDataset(TRAIN_PATH)\n",
    "    val_dataset = SiamesePairDataset(VAL_PATH, \n",
    "                                     a_cols=train_dataset.a_cols, \n",
    "                                     b_cols=train_dataset.b_cols)\n",
    "    test_dataset = SiamesePairDataset(TEST_PATH,\n",
    "                                      a_cols=train_dataset.a_cols,\n",
    "                                      b_cols=train_dataset.b_cols)\n",
    "    \n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim_a = len(train_dataset.a_cols)\n",
    "    input_dim_b = len(train_dataset.b_cols)\n",
    "    \n",
    "    model = SiameseNetwork(\n",
    "        input_dim_a=input_dim_a,\n",
    "        input_dim_b=input_dim_b,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        dropout=DROPOUT,\n",
    "        use_batch_norm=USE_BATCH_NORM\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"\\nModel architecture:\")\n",
    "    print(f\"  Tower A input: {input_dim_a} features\")\n",
    "    print(f\"  Tower B input: {input_dim_b} features\")\n",
    "    print(f\"  Hidden layers: {HIDDEN_DIMS}\")\n",
    "    print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    pos_weight = (len(train_dataset) - train_dataset.y.sum()) / train_dataset.y.sum()\n",
    "    print(f\"\\n⚠️  Class imbalance ratio: {pos_weight:.2f}:1 (negatives:positives)\")\n",
    "    print(f\"   Strategy: {'Focal Loss' if USE_FOCAL_LOSS else 'Weighted BCE Loss'}\")\n",
    "    \n",
    "    if USE_FOCAL_LOSS:\n",
    "        # Focal Loss - better for extreme imbalance\n",
    "        criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "        print(f\"   Focal Loss - alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA}\")\n",
    "        print(f\"   This focuses learning on hard-to-classify examples\")\n",
    "    else:\n",
    "        # Weighted BCE Loss\n",
    "        def weighted_bce_loss(predictions, labels):\n",
    "            loss = -(pos_weight * labels * torch.log(predictions + 1e-7) + \n",
    "                     (1 - labels) * torch.log(1 - predictions + 1e-7))\n",
    "            return loss.mean()\n",
    "        criterion = weighted_bce_loss\n",
    "        print(f\"   Weighted BCE - positive class weighted {pos_weight:.2f}x\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # ✨ NEW: Reduce LR when F1 plateaus (better metric for imbalanced data)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', patience=5, factor=0.5, verbose=True\n",
    "    )  # mode='max' because we want to maximize F1\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    best_val_f1 = 0.0  # ✨ CHANGED: Track best F1 instead of loss\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "    \n",
    "    print(f\"\\nStarting training for {NUM_EPOCHS} epochs...\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
    "    print(f\"Monitoring F1 score for early stopping (patience={PATIENCE})\")\n",
    "    print(f\"Target: F1 > 0.60 for good performance with 9:1 imbalance\\n\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc, train_f1 = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1, val_auc = evaluate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Learning rate scheduling based on F1 (better for imbalanced data)\n",
    "        scheduler.step(val_f1)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Log results\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"  Val Precision: {val_prec:.4f} | Val Recall: {val_rec:.4f}\")\n",
    "        print(f\"  Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # ✨ Warning if model is predicting all one class\n",
    "        if val_f1 < 0.01 and epoch > 3:\n",
    "            print(f\"  ⚠️  WARNING: F1 is near zero - model may be predicting all negatives!\")\n",
    "            print(f\"     Consider: Lower LR, increase focal gamma, or check data\")\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_f1': train_f1,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_precision': val_prec,\n",
    "            'val_recall': val_rec,\n",
    "            'val_f1': val_f1,\n",
    "            'val_auc': val_auc,\n",
    "            'lr': current_lr\n",
    "        })\n",
    "        \n",
    "        # ✨ CHANGED: Save based on F1 score (better for imbalanced data)\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'val_loss': val_loss,\n",
    "                'a_cols': train_dataset.a_cols,\n",
    "                'b_cols': train_dataset.b_cols\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"  → Model saved (best val F1: {best_val_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(RESULTS_PATH, index=False)\n",
    "    print(f\"\\nTraining history saved to {RESULTS_PATH}\")\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    print(\"\\nLoading best model for test evaluation...\")\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # ✨ NEW: Find optimal threshold on validation set\n",
    "    print(\"\\nFinding optimal decision threshold on validation set...\")\n",
    "    optimal_threshold, val_f1_at_threshold = find_optimal_threshold(model, val_loader, DEVICE)\n",
    "    print(f\"Optimal threshold: {optimal_threshold:.3f} (F1: {val_f1_at_threshold:.4f})\")\n",
    "    print(f\"Default 0.5 threshold may not be optimal for imbalanced data!\")\n",
    "    \n",
    "    # Evaluate with both thresholds\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TEST RESULTS (threshold=0.5):\")\n",
    "    print(\"=\"*60)\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1, test_auc = evaluate(\n",
    "        model, test_loader, criterion, DEVICE, threshold=0.5\n",
    "    )\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test Precision: {test_prec:.4f}\")\n",
    "    print(f\"  Test Recall: {test_rec:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"FINAL TEST RESULTS (threshold={optimal_threshold:.3f}):\")\n",
    "    print(\"=\"*60)\n",
    "    test_loss_opt, test_acc_opt, test_prec_opt, test_rec_opt, test_f1_opt, test_auc_opt = evaluate(\n",
    "        model, test_loader, criterion, DEVICE, threshold=optimal_threshold\n",
    "    )\n",
    "    print(f\"  Test Loss: {test_loss_opt:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc_opt:.4f}\")\n",
    "    print(f\"  Test Precision: {test_prec_opt:.4f}\")\n",
    "    print(f\"  Test Recall: {test_rec_opt:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1_opt:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc_opt:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save optimal threshold\n",
    "    checkpoint['optimal_threshold'] = optimal_threshold\n",
    "    torch.save(checkpoint, MODEL_SAVE_PATH)\n",
    "    print(f\"\\n✓ Optimal threshold ({optimal_threshold:.3f}) saved to model checkpoint\")\n",
    "    \n",
    "    return model, history_df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# INFERENCE UTILITIES\n",
    "# ============================================\n",
    "def predict_pairs(model, x_a, x_b, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Predict match probability for new pairs.\n",
    "    \n",
    "    Args:\n",
    "        model: trained SiameseNetwork\n",
    "        x_a: numpy array of A features (n_samples, n_features_a)\n",
    "        x_b: numpy array of B features (n_samples, n_features_b)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: match probabilities (n_samples,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_a_tensor = torch.from_numpy(x_a.astype(np.float32)).to(device)\n",
    "        x_b_tensor = torch.from_numpy(x_b.astype(np.float32)).to(device)\n",
    "        \n",
    "        preds, _, _ = model(x_a_tensor, x_b_tensor)\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_embeddings(model, x_a, x_b, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Get embeddings for rows from both towers.\n",
    "    \n",
    "    Returns:\n",
    "        emb_a, emb_b: numpy arrays of embeddings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_a_tensor = torch.from_numpy(x_a.astype(np.float32)).to(device)\n",
    "        x_b_tensor = torch.from_numpy(x_b.astype(np.float32)).to(device)\n",
    "        \n",
    "        emb_a, emb_b = model.get_embeddings(x_a_tensor, x_b_tensor)\n",
    "        return emb_a.cpu().numpy(), emb_b.cpu().numpy()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION\n",
    "# ============================================\n",
    "def plot_training_history(history_df):\n",
    "    \"\"\"Plot training curves.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history_df['epoch'], history_df['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0, 0].plot(history_df['epoch'], history_df['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training & Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history_df['epoch'], history_df['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[0, 1].plot(history_df['epoch'], history_df['val_acc'], label='Val Acc', marker='s')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Training & Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 0].plot(history_df['epoch'], history_df['val_f1'], label='Val F1', marker='s', color='green')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].set_title('Validation F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # AUC\n",
    "    axes[1, 1].plot(history_df['epoch'], history_df['val_auc'], label='Val AUC', marker='s', color='purple')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('AUC')\n",
    "    axes[1, 1].set_title('Validation AUC')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/training_curves.png\", dpi=150)\n",
    "    print(f\"Training curves saved to {DATA_DIR}/training_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# RUN TRAINING\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    model, history = train_siamese_model()\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    print(f\"✓ Model saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"✓ Results saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
