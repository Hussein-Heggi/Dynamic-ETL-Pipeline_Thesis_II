{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "import pickle\n",
    "from itertools import product\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.stats import hmean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---- model & data paths\n",
    "xgb_model = pickle.load(open('XGboost.pkl', 'rb'))\n",
    "PATH1 = \"../Datasets/Union +/Test 1/coin_Aave.csv\"           # <-- change if needed\n",
    "PATH2 = \"../Datasets/Union +/Test 1/coin_BinanceCoin copy.csv\"    # <-- change if needed\n",
    "OUTPUT_PATH = \"merged_union.csv\"                                                      # <-- output path\n",
    "\n",
    "# ---- validator fusion knobs\n",
    "DECISION_THRESHOLD = 0.72   # final cutoff on fused score (0..1)\n",
    "NAME_WEIGHT       = 0.85    # trust naming more (0..1)\n",
    "MODEL_WEIGHT      = 0.15    # trust model prob (0..1)\n",
    "COMPATIBILITY_CUTOFF = 0.70 # coverage-based score to allow union\n",
    "\n",
    "# ---- MERGE CONFIG (union behavior)\n",
    "ADD_SOURCE_COL      = True          # add __source column indicating origin dataset\n",
    "SOURCE_LABEL_A      = \"A\"           # label for PATH1 rows\n",
    "SOURCE_LABEL_B      = \"B\"           # label for PATH2 rows\n",
    "FAIL_IF_INCOMPATIBLE= False         # if True: raise when compatibility < cutoff\n",
    "COERCE_NUMERIC_MIN_VALID_FRAC = 0.70   # try numeric coercion if >=70% values convert\n",
    "COERCE_DATETIME_MIN_VALID_FRAC = 0.70  # try datetime coercion if >=70% values convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/tmp/ipykernel_73606/3541259347.py:18: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  - converts separators (_ - . / \\ and spaces) to single spaces\n"
     ]
    }
   ],
   "source": [
    "def preprocess_keyword(keyword):\n",
    "    return keyword.replace(\"_\", \" \").lower()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
    "\n",
    "def generate_embeddings(keywords, tokenizer, model, device):\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Aggressive normalization used for union-safe name matching:\n",
    "    - lowercases\n",
    "    - converts separators (_ - . / \\ and spaces) to single spaces\n",
    "    - removes non-alphanumeric chars (except space)\n",
    "    - collapses spaces\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[_\\-\\.\\s/\\\\]+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_set(s: str):\n",
    "    return set(normalize_name(s).split())\n",
    "\n",
    "def is_pure_substring(a_raw: str, b_raw: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if one normalized string is a strict substring of the other (and not equal).\n",
    "    We reject such 'matches' so GDP != GDP per capita.\n",
    "    \"\"\"\n",
    "    a = normalize_name(a_raw).replace(\" \", \"\")\n",
    "    b = normalize_name(b_raw).replace(\" \", \"\")\n",
    "    return (a != b) and (a in b or b in a)\n",
    "\n",
    "def name_similarity(a_raw: str, b_raw: str) -> float:\n",
    "    \"\"\"\n",
    "    Union-safe similarity:\n",
    "      - exact normalized equality â†’ 1.0\n",
    "      - penalize pure substring relations â†’ 0.0\n",
    "      - otherwise blend token Jaccard + character ratio + light length sanity\n",
    "    \"\"\"\n",
    "    a_norm = normalize_name(a_raw)\n",
    "    b_norm = normalize_name(b_raw)\n",
    "\n",
    "    if a_norm == b_norm:\n",
    "        return 1.0\n",
    "\n",
    "    if is_pure_substring(a_raw, b_raw):\n",
    "        return 0.0\n",
    "\n",
    "    char_ratio = difflib.SequenceMatcher(\n",
    "        None, a_norm.replace(\" \", \"\"), b_norm.replace(\" \", \"\")\n",
    "    ).ratio()\n",
    "\n",
    "    A, B = token_set(a_raw), token_set(b_raw)\n",
    "    jacc = len(A & B) / len(A | B) if (A or B) else 0.0\n",
    "\n",
    "    score = 0.6 * jacc + 0.4 * char_ratio\n",
    "\n",
    "    len_ratio = (\n",
    "        min(len(a_norm), len(b_norm)) / max(len(a_norm), len(b_norm))\n",
    "        if max(len(a_norm), len(b_norm)) > 0 else 0\n",
    "    )\n",
    "    score = 0.85 * score + 0.15 * len_ratio\n",
    "    return float(score)\n",
    "\n",
    "def read_headers(csv_path: str):\n",
    "    return list(pd.read_csv(csv_path, nrows=0).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Schemas & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1 columns: 10 | Dataset2 columns: 9\n"
     ]
    }
   ],
   "source": [
    "cols1 = read_headers(PATH1)\n",
    "cols2 = read_headers(PATH2)\n",
    "print(f\"Dataset1 columns: {len(cols1)} | Dataset2 columns: {len(cols2)}\")\n",
    "\n",
    "embeddings_1 = generate_embeddings([preprocess_keyword(c) for c in cols1], tokenizer, model, device)\n",
    "embeddings_2 = generate_embeddings([preprocess_keyword(c) for c in cols2], tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build All Cross-Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_embeddings = []\n",
    "column_pairs    = []\n",
    "for i, e1 in enumerate(embeddings_1):\n",
    "    for j, e2 in enumerate(embeddings_2):\n",
    "        pair_embeddings.append(np.concatenate([e1, e2]))\n",
    "        column_pairs.append((cols1[i], cols2[j]))\n",
    "pair_embeddings = np.vstack(pair_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(pair_embeddings)\n",
    "pair_scaled = scaler.transform(pair_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probs = xgb_model.predict_proba(pair_scaled)[:, 1]\n",
    "name_scores = np.array([name_similarity(a, b) for (a, b) in column_pairs])\n",
    "\n",
    "w_name, w_model = NAME_WEIGHT, MODEL_WEIGHT\n",
    "fused_scores    = w_name * name_scores + w_model * model_probs\n",
    "\n",
    "preds_fused     = (fused_scores >= DECISION_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accepted Pairs & COVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”— Compatible Column Pairs (accepted by manual fusion):\n",
      "- Open â†” Open | fused=0.982 (name=1.000, model=0.881)\n",
      "- Volume â†” Volume | fused=0.975 (name=1.000, model=0.835)\n",
      "- Symbol â†” Symbol | fused=0.964 (name=1.000, model=0.763)\n",
      "- Close â†” Close | fused=0.956 (name=1.000, model=0.706)\n",
      "- High â†” High | fused=0.938 (name=1.000, model=0.588)\n",
      "- SNo â†” SNo | fused=0.896 (name=1.000, model=0.306)\n",
      "- Low â†” Low | fused=0.892 (name=1.000, model=0.283)\n",
      "- Date â†” Date | fused=0.885 (name=1.000, model=0.236)\n",
      "- Name â†” Name | fused=0.859 (name=1.000, model=0.060)\n",
      "Datasets are COMPATIBLE (union-safe)\n",
      "Compatibility Score (coverage-based): 0.95\n"
     ]
    }
   ],
   "source": [
    "compatible_pairs = [\n",
    "    (a, b, s, mp, ns)\n",
    "    for (a, b), s, mp, ns, p in zip(column_pairs, fused_scores, model_probs, name_scores, preds_fused)\n",
    "    if p == 1\n",
    "]\n",
    "\n",
    "matched_A = {a for a, _, _, _, _ in compatible_pairs}\n",
    "matched_B = {b for _, b, _, _, _ in compatible_pairs}\n",
    "\n",
    "coverage_A = len(matched_A) / len(cols1) if cols1 else 0.0\n",
    "coverage_B = len(matched_B) / len(cols2) if cols2 else 0.0\n",
    "compatibility_score = (hmean([coverage_A, coverage_B]) if coverage_A and coverage_B else 0.0)\n",
    "\n",
    "print(\"\\nðŸ”— Compatible Column Pairs (accepted by manual fusion):\")\n",
    "if compatible_pairs:\n",
    "    for a, b, s, mp, ns in sorted(compatible_pairs, key=lambda t: -t[2]):\n",
    "        print(f\"- {a} â†” {b} | fused={s:.3f} (name={ns:.3f}, model={mp:.3f})\")\n",
    "else:\n",
    "    print(\"No compatible columns found.\")\n",
    "\n",
    "is_compatible = (compatibility_score >= COMPATIBILITY_CUTOFF)\n",
    "print(\"Datasets are COMPATIBLE (union-safe)\" if is_compatible else \"Datasets are NOT compatible (union-safe)\")\n",
    "print(f\"Compatibility Score (coverage-based): {compatibility_score:.2f}\")\n",
    "\n",
    "# >>> HARD GUARD: abort before any union steps\n",
    "if not is_compatible:\n",
    "    print(f\"\\nâ›” Union aborted: compatibility {compatibility_score:.2f} is below cutoff {COMPATIBILITY_CUTOFF:.2f}.\")\n",
    "    sys.exit(2)   # non-zero exit so pipelines can detect the abort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-To-One Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Selected 1:1 column mapping (Dataset2 â†’ Dataset1):\n",
      "- Open  â†’  Open\n",
      "- Volume  â†’  Volume\n",
      "- Symbol  â†’  Symbol\n",
      "- Close  â†’  Close\n",
      "- High  â†’  High\n",
      "- SNo  â†’  SNo\n",
      "- Low  â†’  Low\n",
      "- Date  â†’  Date\n",
      "- Name  â†’  Name\n"
     ]
    }
   ],
   "source": [
    "# We select a 1:1 mapping from Dataset2->Dataset1, highest fused first.\n",
    "pairs_sorted = sorted(compatible_pairs, key=lambda t: -t[2])\n",
    "A_used, B_used = set(), set()\n",
    "mapping_B_to_A = {}   # {B_col: A_col}\n",
    "\n",
    "for a, b, s, _, _ in pairs_sorted:\n",
    "    if a not in A_used and b not in B_used:\n",
    "        mapping_B_to_A[b] = a\n",
    "        A_used.add(a)\n",
    "        B_used.add(b)\n",
    "\n",
    "print(\"\\nâœ… Selected 1:1 column mapping (Dataset2 â†’ Dataset1):\")\n",
    "if mapping_B_to_A:\n",
    "    for b, a in mapping_B_to_A.items():\n",
    "        print(f\"- {b}  â†’  {a}\")\n",
    "else:\n",
    "    print(\"No 1:1 mappings selected (union will still proceed with unmatched columns).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(PATH1, low_memory=False)\n",
    "df2 = pd.read_csv(PATH2, low_memory=False)\n",
    "\n",
    "# Provenance (optional)\n",
    "if ADD_SOURCE_COL:\n",
    "    df1[\"__source\"] = SOURCE_LABEL_A\n",
    "    df2[\"__source\"] = SOURCE_LABEL_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Dataset 2 Columns Usings Selected Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns=mapping_B_to_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D-Type Harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
      "/tmp/ipykernel_73606/3791693906.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "def try_coerce_numeric(s: pd.Series) -> (pd.Series, float):\n",
    "    coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "    valid_frac = 1.0 - (coerced.isna() & s.notna()).mean()\n",
    "    return coerced, valid_frac\n",
    "\n",
    "def try_coerce_datetime(s: pd.Series) -> (pd.Series, float):\n",
    "    coerced = pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
    "    valid_frac = 1.0 - (coerced.isna() & s.notna()).mean()\n",
    "    return coerced, valid_frac\n",
    "\n",
    "def harmonize_pair(col: str, dfa: pd.DataFrame, dfb: pd.DataFrame):\n",
    "    sa = dfa[col]\n",
    "    sb = dfb[col]\n",
    "    # Attempt numeric\n",
    "    ca, fa = try_coerce_numeric(sa)\n",
    "    cb, fb = try_coerce_numeric(sb)\n",
    "    if fa >= COERCE_NUMERIC_MIN_VALID_FRAC and fb >= COERCE_NUMERIC_MIN_VALID_FRAC:\n",
    "        dfa[col] = ca.astype(\"float64\")\n",
    "        dfb[col] = cb.astype(\"float64\")\n",
    "        return\n",
    "\n",
    "    # Attempt datetime\n",
    "    ca, fa = try_coerce_datetime(sa)\n",
    "    cb, fb = try_coerce_datetime(sb)\n",
    "    if fa >= COERCE_DATETIME_MIN_VALID_FRAC and fb >= COERCE_DATETIME_MIN_VALID_FRAC:\n",
    "        dfa[col] = ca\n",
    "        dfb[col] = cb\n",
    "        return\n",
    "\n",
    "    # Fallback: object (string)\n",
    "    dfa[col] = dfa[col].astype(\"object\")\n",
    "    dfb[col] = dfb[col].astype(\"object\")\n",
    "\n",
    "# Harmonize dtypes on intersecting columns (after renaming)\n",
    "common_cols = [c for c in df1.columns if c in df2.columns]\n",
    "for c in common_cols:\n",
    "    harmonize_pair(c, df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Schema & Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Dataset1 columns first, then any extra columns from Dataset2\n",
    "union_cols = list(df1.columns) + [c for c in df2.columns if c not in df1.columns]\n",
    "\n",
    "df1u = df1.reindex(columns=union_cols)\n",
    "df2u = df2.reindex(columns=union_cols)\n",
    "\n",
    "df_merged = pd.concat([df1u, df2u], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Union summary:\n",
      "- Rows: A=275, B=1442, merged=1717\n",
      "- Columns in both: 10\n",
      "- Columns only in A: 1\n",
      "- Columns only in B (after rename): 0\n",
      "\n",
      "ðŸ’¾ Saved merged dataset to: merged_union.csv\n"
     ]
    }
   ],
   "source": [
    "only_in_A = [c for c in df1.columns if c not in df2.columns]\n",
    "only_in_B_after_rename = [c for c in df2.columns if c not in df1.columns]\n",
    "both = [c for c in df1.columns if c in df2.columns]\n",
    "\n",
    "print(\"\\nðŸ“¦ Union summary:\")\n",
    "print(f\"- Rows: A={len(df1)}, B={len(df2)}, merged={len(df_merged)}\")\n",
    "print(f\"- Columns in both: {len(both)}\")\n",
    "print(f\"- Columns only in A: {len(only_in_A)}\")\n",
    "print(f\"- Columns only in B (after rename): {len(only_in_B_after_rename)}\")\n",
    "\n",
    "df_merged.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\nðŸ’¾ Saved merged dataset to: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
