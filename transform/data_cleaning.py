# -*- coding: utf-8 -*-
"""data_cleaning_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11bQED3r08V5V1om4pk0J5aVZ65r9hmAZ
"""

from __future__ import annotations
import pandas as pd
import numpy as np
from typing import Tuple, Union, Dict, Any
import logging

# Optional: Configure logging to see reports in your console/notebook output
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

REQUIRED_COLS = ["ticker", "ts", "open", "high", "low", "close", "volume"]


def clean_stock_bars(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    No-exception cleaner:
     - keeps tz-aware UTC in 'ts'
     - drops exact duplicate rows only
     - hard-drops invalid rows (future ts, negative/zero prices, bad high/low, negative volume)
     - soft-fixes vwap (set NaN if out of [low, high]) and transactions (nullable Int64, <0 -> NA)
     - returns (clean_df, report)
    """
    rep: Dict[str, Any] = {"clean": {}}
    d = df.copy()

    # Ensure required columns exist
    missing = [c for c in REQUIRED_COLS if c not in d.columns]
    if missing:
        rep["clean"]["missing_required_columns"] = missing
        empty_cols = list(dict.fromkeys([*REQUIRED_COLS, *d.columns]))
        return pd.DataFrame(columns=empty_cols), rep

    # Parse/normalize types
    d["ts"] = pd.to_datetime(d["ts"], errors="coerce", utc=True)
    for c in ["open", "high", "low", "close", "volume"]:
        d[c] = pd.to_numeric(d[c], errors="coerce")

    if "vwap" in d.columns:
        d["vwap"] = pd.to_numeric(d["vwap"], errors="coerce")
    if "transactions" in d.columns:
        d["transactions"] = pd.to_numeric(d["transactions"], errors="coerce").astype(
            "Int64"
        )

    # Drop exact duplicates only
    before = len(d)
    d = d.drop_duplicates(keep="first")
    rep["clean"]["exact_duplicates_dropped"] = int(before - len(d))

    # Hard filters
    now_utc = pd.Timestamp.now(tz="UTC")
    oc_max = d[["open", "close"]].max(axis=1)
    oc_min = d[["open", "close"]].min(axis=1)

    keep_mask = (
        d[["ticker", "ts", "open", "high", "low", "close", "volume"]]
        .notna()
        .all(axis=1)
        & (d["ts"] <= now_utc)
        & (d["open"] > 0)
        & (d["high"] > 0)
        & (d["low"] > 0)
        & (d["close"] > 0)
        & (d["high"] >= oc_max)
        & (d["low"] <= oc_min)
        & (d["volume"] >= 0)
    )
    rep["clean"]["hard_rows_dropped"] = int((~keep_mask).sum())
    d = d.loc[keep_mask].copy()

    # Soft fixes
    if "vwap" in d.columns:
        bad_vwap = (~d["vwap"].isna()) & (
            (d["vwap"] < d["low"]) | (d["vwap"] > d["high"])
        )
        rep["clean"]["vwap_set_null"] = int(bad_vwap.sum())
        d.loc[bad_vwap, "vwap"] = np.nan

    if "transactions" in d.columns:
        bad_tx = d["transactions"].notna() & (d["transactions"] < 0)
        rep["clean"]["transactions_set_null"] = int(bad_tx.sum())
        d.loc[bad_tx, "transactions"] = pd.NA
        d["transactions"] = d["transactions"].astype("Int64")

    # Optional tidy & dtype align
    d = d.sort_values(["ticker", "ts"], kind="stable").reset_index(drop=True)

    return d, rep


def get_default_schema_config() -> Dict[str, Any]:
    """Returns the default validation schema configuration for stock bars."""
    import pandera as pa
    from pandera import Column, Check

    # Vectorized checks for better performance
    is_tz_aware = Check(
        lambda s: pd.api.types.is_datetime64_any_dtype(s) and s.dt.tz is not None,
        element_wise=False,
        error="ts must be a timezone-aware Series.",
    )
    is_utc = Check(
        lambda s: str(s.dt.tz) in ("UTC", "utc"),
        element_wise=False,
        error="ts must be UTC.",
    )
    is_not_future = Check(
        lambda s: (s <= pd.Timestamp.now(tz="UTC")).all(),
        element_wise=False,
        error="Found future timestamps.",
    )

    schema_config = {
        "columns": {
            "ticker": Column(pa.String, checks=Check.str_length(min_value=1)),
            # THIS IS THE FIX: Directly specify the pandas dtype string for a timezone-aware column.
            "ts": Column(
                dtype="datetime64[ns, UTC]", checks=[is_tz_aware, is_utc, is_not_future]
            ),
            "open": Column(
                pa.Float, checks=[Check.gt(0), Check(lambda s: np.isfinite(s))]
            ),
            "high": Column(
                pa.Float, checks=[Check.gt(0), Check(lambda s: np.isfinite(s))]
            ),
            "low": Column(
                pa.Float, checks=[Check.gt(0), Check(lambda s: np.isfinite(s))]
            ),
            "close": Column(
                pa.Float, checks=[Check.gt(0), Check(lambda s: np.isfinite(s))]
            ),
            "volume": Column(pa.Int, checks=Check.ge(0)),
            "vwap": Column(pa.Float, nullable=True, required=False),
            "transactions": Column(
                pa.Int, nullable=True, checks=Check.ge(0), required=False
            ),
        },
        "dataframe_checks": [
            Check(
                lambda df: (df["high"] >= df[["open", "close"]].max(axis=1)).all(),
                error="high < max(open, close).",
            ),
            Check(
                lambda df: (df["low"] <= df[["open", "close"]].min(axis=1)).all(),
                error="low > min(open, close).",
            ),
            Check(
                lambda df: ("vwap" not in df.columns)
                or (
                    (df["vwap"].isna())
                    | ((df["vwap"] >= df["low"]) & (df["vwap"] <= df["high"]))
                ).all(),
                error="vwap must be within [low, high] when present.",
            ),
        ],
    }
    return schema_config


def pandera_validate_safely(
    df: pd.DataFrame, schema_config: Dict[str, Any] = None
) -> Dict[str, Any]:
    """If pandera is installed, validate using a flexible schema. Never raises."""
    info: Dict[str, Any] = {"pandera": {"enabled": False, "status": "skipped"}}
    try:
        import pandera as pa
    except Exception as e:
        info["pandera"] = {
            "enabled": False,
            "status": f"not_installed_or_import_failed: {e}",
        }
        return info

    if schema_config is None:
        schema_config = get_default_schema_config()

    columns_to_validate = {
        col_name: col_schema
        for col_name, col_schema in schema_config["columns"].items()
        if col_name in df.columns
    }

    schema = pa.DataFrameSchema(
        columns=columns_to_validate,
        checks=schema_config.get("dataframe_checks", []),
        coerce=False,
        strict=False,
        unique=None,
    )

    _v = df.copy()

    try:
        schema.validate(_v, lazy=True)
        info["pandera"] = {"enabled": True, "status": "passed"}
    except Exception as e:
        info["pandera"] = {
            "enabled": True,
            "status": f"failed: {type(e).__name__}",
            "details": str(e),
        }
    return info


def pipeline_clean(
    data: Union[str, pd.DataFrame],
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Accepts CSV path or DataFrame.
    Returns (cleaned_df, report). Never raises.
    """
    report: Dict[str, Any] = {}

    # Read data
    if isinstance(data, str):
        try:
            df = pd.read_csv(
                data, skipinitialspace=True, engine="python", on_bad_lines="skip"
            )
            report["read"] = {
                "path": data,
                "rows": len(df),
                "columns": list(df.columns),
                "on_bad_lines": "skip",
            }
        except Exception as e:
            report["read"] = {"path": data, "error": str(e)}
            return pd.DataFrame(), report
    else:
        df = data.copy()
        report["read"] = {"path": None, "rows": len(df), "columns": list(df.columns)}

    # Clean data
    cleaned, clean_rep = clean_stock_bars(df)
    report.update(clean_rep)

    # Validate with Pandera
    pandera_rep = pandera_validate_safely(cleaned)
    report.update(pandera_rep)

    # Log the final report for easy viewing
    logging.info(f"Pipeline Report: {report}")

    return cleaned, report


# clean_df, report = pipeline_clean("data_cleaning_dirty.csv")
# print(clean_df)
# print(report)
